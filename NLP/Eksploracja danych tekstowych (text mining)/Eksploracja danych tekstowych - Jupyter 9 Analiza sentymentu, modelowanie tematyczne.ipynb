{"cells":[{"cell_type":"markdown","metadata":{"id":"H-BScvbg_M_7"},"source":["### 1. Importowanie oraz pobieranie modułów"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"03lBAycX_M_7"},"outputs":[],"source":["import nltk"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"ZGLGx22C_M_9"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     /Users/mgprivate/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# po pobraniu VADERA powinieneś zobaczyć \"True\", jeżeli nie możesz sciągnąć tego modelu, spróbuj odblokować dostęp w swoim Firewallu\n","nltk.download(\"vader_lexicon\")"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"_3ivACu-_M_9"},"outputs":[],"source":["# moduł można zaimportować dopiero po pobraniu modelu\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer"]},{"cell_type":"markdown","metadata":{"id":"-FYNRZk-_M_-"},"source":["### 2. Wykorzystanie modelu Vader"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"Icfhk_bXx8ew"},"outputs":[{"data":{"text/plain":["{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# tworzenie instancji analizatora sentymentu\n","sid = SentimentIntensityAnalyzer()\n","sentence = input(\"wprowadź swoje zdanie tutaj\")\n","\n","# nacechowanie tekstu będzie widoczne jako wartość dla klucza 'compound'\n","sid.polarity_scores(sentence)"]},{"cell_type":"markdown","metadata":{"id":"bWsYrEz6ySPl"},"source":["### 3. Modelowanie tematyczne (topic modelling)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"0aaBtDidyaav"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","import pandas as pd\n","\n","# load the dataset\n","# dataset https://gist.github.com/kunalj101/ad1d9c58d338e20d09ff26bcc06c4235\n","data = open('corpus').read()\n","labels, texts = [], []\n","for i, line in enumerate(data.split(\"\\n\")):\n","    content = line.split()\n","    labels.append(content[0])\n","    texts.append(\" \".join(content[1:]))\n","\n","# create a dataframe using texts and lables\n","trainDF = pd.DataFrame()\n","trainDF['text'] = texts\n","trainDF['label'] = labels"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"gcwVhmuRyamG"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","\n","# split the dataset into training and validation datasets\n","train_x, valid_x, train_y, valid_y = train_test_split(trainDF['text'], trainDF['label'])\n","\n","# label encode the target variable\n","encoder = preprocessing.LabelEncoder()\n","train_y = encoder.fit_transform(train_y)\n","valid_y = encoder.fit_transform(valid_y)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"r28dtJGQya99"},"outputs":[{"name":"stdout","output_type":"stream","text":["  (0, 710)\t1\n","  (0, 1743)\t1\n","  (0, 2752)\t1\n","  (0, 3725)\t1\n","  (0, 4138)\t1\n","  (0, 4423)\t4\n","  (0, 5175)\t4\n","  (0, 5305)\t2\n","  (0, 5306)\t1\n","  (0, 8139)\t1\n","  (0, 8143)\t1\n","  (0, 8711)\t1\n","  (0, 8755)\t4\n","  (0, 9442)\t1\n","  (0, 11064)\t1\n","  (0, 11117)\t1\n","  (0, 11390)\t1\n","  (0, 13407)\t4\n","  (0, 14104)\t1\n","  (0, 15169)\t2\n","  (0, 15569)\t3\n","  (0, 16193)\t1\n","  (0, 16953)\t1\n","  (0, 19529)\t1\n","  (0, 19630)\t1\n","  :\t:\n","  (7499, 17802)\t2\n","  (7499, 18833)\t1\n","  (7499, 18847)\t1\n","  (7499, 19039)\t1\n","  (7499, 19240)\t2\n","  (7499, 19525)\t1\n","  (7499, 19529)\t1\n","  (7499, 19647)\t1\n","  (7499, 19752)\t2\n","  (7499, 21901)\t1\n","  (7499, 23878)\t1\n","  (7499, 24791)\t1\n","  (7499, 25337)\t1\n","  (7499, 26296)\t1\n","  (7499, 28082)\t7\n","  (7499, 28215)\t1\n","  (7499, 28224)\t1\n","  (7499, 28312)\t1\n","  (7499, 28493)\t3\n","  (7499, 30167)\t1\n","  (7499, 30607)\t1\n","  (7499, 30682)\t1\n","  (7499, 31250)\t1\n","  (7499, 31506)\t1\n","  (7499, 31521)\t1\n"]}],"source":["# create a count vectorizer object\n","count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n","count_vect.fit(trainDF['text'])\n","\n","# transform the training and validation data using count vectorizer object\n","xtrain_count =  count_vect.transform(train_x)\n","xvalid_count =  count_vect.transform(valid_x)\n","print(xtrain_count)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"Cd3-olwUvtOO"},"outputs":[{"name":"stdout","output_type":"stream","text":["['toy jack installed software scanner elements x pet haiku advanced', 'the of book a and is in to this read', 'player software mp3 usb button sale files file 45 function', 'la de y un en el overrated et que a', 'prehistoric snmp march rockin clarity stages courses trained gnostic boston', 'rice action skin movie david van gonna ii ball special', 'turns bradley napoleon lake twists theology loosely michigan titan interactions', 'desk salt factual academic grain campaign strangers locate rewarded communication', 'manson illustrations run ready soundtrack returning german wall lola saved', 'the i it and to a this is of for', 'dolly bluegrass packaged cooking argento lee patterns sloppy troma kitchen', 'voodoo havent integrity dcr joanna misled divinity nicoletta candid decameron', 'product works price replacement computer apple power support plug shipping', 'printer hp thin print paper genius diane lane drive error', 'battery labor anti produced whose henry outlet co beds eargels', 'helpful sex public test variety practice speaking emarker detailed classes', 'album town jazz doubt titanic inspiring produce yellow wayne spiritual', 'bd crawford cookbook patches vegan rashel gut borders intricate bond', 'java emotions core tray breathtaking jet rome captures bow sung', 'propaganda bikes woods wing organization parables puppy misses agenda scripture']\n"]}],"source":["from sklearn import decomposition\n","import numpy\n","\n","\n","# train a LDA Model\n","lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n","X_topics = lda_model.fit_transform(xtrain_count)\n","topic_word = lda_model.components_\n","vocab = count_vect.get_feature_names()\n","\n","# view the topic models\n","n_top_words = 10\n","topic_summaries = []\n","for i, topic_dist in enumerate(topic_word):\n","    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n","    topic_summaries.append(' '.join(topic_words))\n","print(topic_summaries)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
